{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Colors\n",
    "using Images\n",
    "using DataFrames\n",
    "using HDF5\n",
    "using MLBase\n",
    "\n",
    "use_gpu = false\n",
    "if use_gpu\n",
    "    ENV[\"MOCHA_USE_CUDA\"] = \"true\"\n",
    "else\n",
    "    ENV[\"MOCHA_USE_NATIVE_EXT\"] = \"true\"\n",
    "    blas_set_num_threads(2)\n",
    "end\n",
    "\n",
    "using Mocha\n",
    "srand(333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set: (6283,2)\n",
      "Size of test data set: (6220,2)\n"
     ]
    }
   ],
   "source": [
    "imageSize = (20,20,1) # 20 x 20 pixel x 1 color\n",
    "\n",
    "#Set location of data files, folders\n",
    "path = \"data\"\n",
    "\n",
    "#Read information about training data , IDs.\n",
    "labelsInfo = readtable(\"$(path)/trainLabels.csv\")\n",
    "\n",
    "println(\"Size of data set: \", size(labelsInfo))\n",
    "\n",
    "labelsInfoTest = readtable(\"$(path)/sampleSubmission.csv\")\n",
    "\n",
    "println(\"Size of test data set: \", size(labelsInfoTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><tr><th></th><th>ID</th><th>Class</th><th>Labels</th></tr><tr><th>1</th><td>1</td><td>n</td><td>0</td></tr><tr><th>2</th><td>2</td><td>8</td><td>1</td></tr><tr><th>3</th><td>3</td><td>T</td><td>2</td></tr><tr><th>4</th><td>4</td><td>I</td><td>3</td></tr><tr><th>5</th><td>5</td><td>R</td><td>4</td></tr><tr><th>6</th><td>6</td><td>W</td><td>5</td></tr></table>"
      ],
      "text/plain": [
       "6x3 DataFrame\n",
       "| Row | ID | Class | Labels |\n",
       "|-----|----|-------|--------|\n",
       "| 1   | 1  | \"n\"   | 0      |\n",
       "| 2   | 2  | \"8\"   | 1      |\n",
       "| 3   | 3  | \"T\"   | 2      |\n",
       "| 4   | 4  | \"I\"   | 3      |\n",
       "| 5   | 5  | \"R\"   | 4      |\n",
       "| 6   | 6  | \"W\"   | 5      |"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need labels from 0 to N-1 for Mocha\n",
    "labs = unique(labelsInfo[:Class])\n",
    "dic = Dict(zip(collect(labs), 0:length(labs)-1))\n",
    "create_labs(classes) = map(k -> dic[k], classes)\n",
    "labelsInfo[:Labels] = create_labs(labelsInfo[:Class])\n",
    "labelsInfoTest[:Labels] = create_labs(labelsInfoTest[:Class])\n",
    "head(labelsInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on train and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train data set: (4713,3)\n",
      "Size of the validation data set: (785,3)\n",
      "Size of the holdout data set: (785,3)\n"
     ]
    }
   ],
   "source": [
    "srand(12345)\n",
    "n = length(labelsInfo[:ID])\n",
    "trainSet = shuffle(1:n .> n*0.25)\n",
    "labelsInfoTrain = labelsInfo[trainSet,:]\n",
    "\n",
    "# Hold out some data from validation set\n",
    "srand(12345)\n",
    "n = length(labelsInfo[!trainSet,:ID])\n",
    "validationSet = shuffle(1:n .> n*0.5)\n",
    "labelsInfoValid = labelsInfo[!trainSet,:][validationSet,:]\n",
    "labelsInfoHoldout = labelsInfo[!trainSet,:][!validationSet,:]\n",
    "\n",
    "println(\"Size of the train data set: \", size(labelsInfoTrain))\n",
    "println(\"Size of the validation data set: \", size(labelsInfoValid))\n",
    "println(\"Size of the holdout data set: \", size(labelsInfoHoldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images from the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readImages (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readImages(typeData, labelsInfo, imageSize, path)\n",
    "    w, h, c = imageSize\n",
    "    n = length(labelsInfo[:ID])\n",
    "    x = float32(zeros(w,h,c,n))\n",
    "    for (index, idImage) in enumerate(labelsInfo[:ID]) \n",
    "        #Read image file \n",
    "        nameFile = \"$(path)/$(typeData)Resized/$(idImage).Bmp\"\n",
    "        img = imread(nameFile)\n",
    "\n",
    "        #Convert img to float values \n",
    "        img = convert(Array{Gray}, img)\n",
    "        img = convert(Array{Float32}, img)\n",
    "        #img = separate(convert(Image{RGB}, img))\n",
    "        #img = convert(Array, img)\n",
    "        #img = img[:,:,1:3]\n",
    "        \n",
    "        # Normalize data\n",
    "        img = (img - mean(img)) / std(img)\n",
    "        x[:,:,:,index] = reshape(img, w, h, c)\n",
    "        \n",
    "    end \n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train images set: (20,20,1,4713)\n",
      "Size of the validation images set: (20,20,1,785)\n",
      "Size of the holdout images set: (20,20,1,785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the test images set: (20,20,1,6220)\n"
     ]
    }
   ],
   "source": [
    "trainImages = readImages(\"train\", labelsInfoTrain, imageSize, path)\n",
    "println(\"Size of the train images set: \", size(trainImages))\n",
    "\n",
    "validImages = readImages(\"train\", labelsInfoValid, imageSize, path)\n",
    "println(\"Size of the validation images set: \", size(validImages))\n",
    "\n",
    "holdoutImages = readImages(\"train\", labelsInfoHoldout, imageSize, path)\n",
    "println(\"Size of the holdout images set: \", size(validImages))\n",
    "\n",
    "testImages = readImages(\"test\", labelsInfoTest, imageSize, path)\n",
    "println(\"Size of the test images set: \", size(testImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images into HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data to HDF5 format\n",
    "function convert_to_HDF5(typeData, imageSet, labelsInfo, path)\n",
    "    w, h, c, n = size(imageSet)\n",
    "    \n",
    "    HDF5.h5open(\"$(path)/$(typeData).hdf5\", \"w\") do h5\n",
    "        dset_data = d_create(h5, \"data\", datatype(Float32), dataspace(w, h, c, n))\n",
    "        dset_data[:,:,:,:] =  imageSet\n",
    "        \n",
    "        dset_label = d_create(h5, \"label\", datatype(Float32), dataspace(1,n))\n",
    "        dset_label[1,:] = labelsInfo[:Labels]\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "convert_to_HDF5(\"train\", trainImages, labelsInfoTrain, path)\n",
    "convert_to_HDF5(\"validation\", validImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"holdout\", holdoutImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"test\", testImages, labelsInfoTest, path)\n",
    "\n",
    "run(`echo $(path)/train.hdf5` |> \"$(path)/train.txt\")\n",
    "run(`echo $(path)/validation.hdf5` |> \"$(path)/validation.txt\")\n",
    "run(`echo $(path)/holdout.hdf5` |> \"$(path)/holdout.txt\")\n",
    "run(`echo $(path)/test.hdf5` |> \"$(path)/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search will run 1 times\n",
      "Solver will run for 4230 iterations\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH = 100\n",
    "EPOCH = int(round(size(trainImages)[4] / TRAIN_BATCH))\n",
    "MAXITER = 90*EPOCH\n",
    "\n",
    "NCLASSES = length(unique(labelsInfoTrain[:Class]))\n",
    "nunits_fc1 = (\"nunits_fc1\", [2400])\n",
    "nunits_fc2 = (\"nunits_fc2\", [1200])\n",
    "base_mom = (\"base_mom\", [0.95])\n",
    "base_lr = (\"base_lr\", [0.01])\n",
    "regu_coef = (\"regu_coef\", [0.0])\n",
    "conv1_nfilt = (\"conv1_nfilt\", [96])\n",
    "conv2_nfilt = (\"conv2_nfilt\", [128])\n",
    "\n",
    "n = length(nunits_fc1[2]) * length(nunits_fc2[2]) * length(base_mom[2]) * length(base_lr[2]) * length(regu_coef[2]) * length(conv1_nfilt[2]) * length(conv2_nfilt[2])\n",
    "println(\"Grid search will run \", n, \" times\")\n",
    "println(\"Solver will run for \", MAXITER, \" iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Neural Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_training (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_training(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt)\n",
    "    data_layer  = AsyncHDF5DataLayer(name=\"train-data\", source=\"$(path)/train.txt\", \n",
    "                                    batch_size=TRAIN_BATCH)\n",
    "    conv_layer  = ConvolutionLayer(name=\"conv1\", n_filter=conv1_nfilt, kernel=(5,5), bottoms=[:data], tops=[:conv])\n",
    "    pool_layer  = PoolingLayer(name=\"pool1\", kernel=(2,2), stride=(2,2), bottoms=[:conv], tops=[:pool])\n",
    "    conv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=conv2_nfilt, kernel=(5,5), bottoms=[:pool], tops=[:conv2])\n",
    "    pool2_layer = PoolingLayer(name=\"pool2\", kernel=(2,2), stride=(2,2), bottoms=[:conv2], tops=[:pool2])\n",
    "    fc1_layer   = InnerProductLayer(name=\"ip1\", output_dim=nunits_fc1, neuron=Neurons.ReLU(), bottoms=[:pool2], tops=[:ip1])\n",
    "    fc2_layer   = InnerProductLayer(name=\"ip2\", output_dim=nunits_fc2, neuron=Neurons.ReLU(), bottoms=[:ip1], tops=[:ip2])\n",
    "    fc3_layer   = InnerProductLayer(name=\"ip3\", output_dim=NCLASSES, bottoms=[:ip2], tops=[:out])\n",
    "    loss_layer  = SoftmaxLossLayer(name=\"loss\", bottoms=[:out,:label])\n",
    "    \n",
    "    backend = use_gpu ? GPUBackend() : CPUBackend()\n",
    "    init(backend)\n",
    "\n",
    "    # setup dropout for the different layers\n",
    "    # we use 20% dropout on the inputs and 50% dropout in the hidden layers\n",
    "    # as these values were previously found to be good defaults\n",
    "    drop_input = DropoutLayer(name=\"drop_in\", bottoms=[:data], ratio=0.1)\n",
    "    drop_conv1 = DropoutLayer(name=\"drop_conv1\", bottoms=[:pool], ratio=0.2)\n",
    "    drop_conv2 = DropoutLayer(name=\"drop_conv2\", bottoms=[:pool2], ratio=0.2)\n",
    "    drop_ip1 = DropoutLayer(name=\"drop_ip1\", bottoms=[:ip1], ratio=0.5)\n",
    "    drop_ip2= DropoutLayer(name=\"drop_ip2\", bottoms=[:ip2], ratio=0.5)\n",
    "\n",
    "    common_layers = [conv_layer, pool_layer, conv2_layer, pool2_layer, fc1_layer, fc2_layer, fc3_layer]\n",
    "    drop_layers = [drop_input, drop_conv1, drop_conv2, drop_ip1, drop_ip2]\n",
    "    # put training net together, note that the correct ordering will automatically be established by the constructor\n",
    "    net = Net(\"SVHN-train\", backend, [data_layer, common_layers..., drop_layers..., loss_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    full_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"train-full-data\", \n",
    "        source=\"$(path)/train.txt\", \n",
    "        batch_size=size(testImages)[4])\n",
    "    full_acc_layer = AccuracyLayer(name=\"full_train\", bottoms=[:out, :label], report_error=true)\n",
    "    train_net = Net(\"SVHN-train-prediction\", backend, [full_data_layer, common_layers..., full_acc_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    valid_batch = size(validImages)[4]\n",
    "    valid_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"validation-data\", \n",
    "        source=\"$(path)/validation.txt\", \n",
    "        batch_size=valid_batch)\n",
    "    valid_acc_layer = AccuracyLayer(name=\"validation\", bottoms=[:out, :label], report_error=true)\n",
    "    valid_net = Net(\"SVHN-validation-prediction\", backend, [valid_data_layer, common_layers..., valid_acc_layer])\n",
    "    \n",
    "    println(net)\n",
    "    #println(valid_net)\n",
    "    return(net, train_net, valid_net) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_solver(niter, base_mom, base_lr, epoch, base_dir, regu_coef=0.0)\n",
    "    params = SolverParameters(\n",
    "                max_iter=niter,\n",
    "                regu_coef=regu_coef,\n",
    "                mom_policy=MomPolicy.Fixed(base_mom),\n",
    "                lr_policy=LRPolicy.Inv(base_lr, 0.0001, 0.75), \n",
    "                load_from=base_dir)\n",
    "\n",
    "    solver = SGD(params)\n",
    "    \n",
    "    return(solver)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup coffee breaks for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_coffebreaks (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_coffebreaks(solver, train_net, valid_net, base_dir)\n",
    "    setup_coffee_lounge(solver, save_into=\"$(base_dir)/statistics.jld\", every_n_iter=5000)\n",
    "\n",
    "    # report training progress every 100 iterations\n",
    "    add_coffee_break(solver, TrainingSummary(show_lr=true, show_mom=true), every_n_iter=100)\n",
    "\n",
    "    # Report train perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(train_net), every_n_iter=500)\n",
    "    \n",
    "    # Report validation perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=500)\n",
    "    \n",
    "    # save snapshots every 1000 iterations\n",
    "    add_coffee_break(solver, Snapshot(base_dir), every_n_iter=1000) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure grid serach params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(predict_net::Net, base_dir::String) \n",
    "    load_snapshot(predict_net, base_dir)\n",
    "    \n",
    "    init(predict_net)\n",
    "    forward_epoch(predict_net)\n",
    "\n",
    "    batch = []\n",
    "    if isa(predict_net.states[end].layer, AccuracyLayer)\n",
    "        batch = to_array(predict_net.states[end-1].blobs[1])\n",
    "    else \n",
    "        batch = to_array(predict_net.states[end].blobs[1])\n",
    "    end\n",
    "    \n",
    "    n = size(batch)[2]\n",
    "    pred = zeros(n)\n",
    "    for i in 1:n\n",
    "        pred[i] = indmax(batch[:,i]) - 1\n",
    "    end\n",
    "    \n",
    "    return(pred)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalfun (generic function with 1 method)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evalfun(netInfo)\n",
    "    pred = predict(netInfo[:valid_net], netInfo[:base_dir])\n",
    "    model_perfomance = mean(pred .== netInfo[:validLabels])\n",
    "    \n",
    "    return(model_perfomance)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estfun (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function estfun(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt, base_mom, base_lr, regu_coef)\n",
    "    snapshot_dir = \"snapshot_drop_conv_$(conv1_nfilt)_$(conv2_nfilt)_$(nunits_fc1)_$(nunits_fc2)_$(base_mom)_$(base_lr)_$(regu_coef)\"\n",
    "    net, train_net, valid_net = configure_training(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt)\n",
    "    solver = configure_solver(MAXITER, base_mom, base_lr, EPOCH, snapshot_dir, regu_coef)\n",
    "    configure_coffebreaks(solver, train_net, valid_net, snapshot_dir)\n",
    "    solve(solver, net) \n",
    "    model = {:net => net, \n",
    "             :valid_net => valid_net,\n",
    "             :base_dir => snapshot_dir, \n",
    "             :validLabels => labelsInfoValid[:Labels]}\n",
    "    \n",
    "    return(model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-Aug 21:50:36:INFO:root:Constructing net SVHN-train on CPUBackend...\n",
      "25-Aug 21:50:36:INFO:root:Topological sorting 14 layers...\n",
      "25-Aug 21:50:36:INFO:root:Setup layers...\n",
      "25-Aug 21:50:36:INFO:root:Network constructed!\n",
      "25-Aug 21:50:36:INFO:root:Constructing net SVHN-train-prediction on CPUBackend...\n",
      "25-Aug 21:50:36:INFO:root:Topological sorting 9 layers...\n",
      "25-Aug 21:50:36:INFO:root:Setup layers...\n",
      "25-Aug 21:50:36:DEBUG:root:ConvolutionLayer(conv1): sharing filters and bias\n",
      "25-Aug 21:50:36:DEBUG:root:ConvolutionLayer(conv2): sharing filters and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "25-Aug 21:50:36:INFO:root:Network constructed!\n",
      "25-Aug 21:50:36:INFO:root:Constructing net SVHN-validation-prediction on CPUBackend...\n",
      "25-Aug 21:50:36:INFO:root:Topological sorting 9 layers...\n",
      "25-Aug 21:50:36:INFO:root:Setup layers...\n",
      "25-Aug 21:50:36:DEBUG:root:ConvolutionLayer(conv1): sharing filters and bias\n",
      "25-Aug 21:50:36:DEBUG:root:ConvolutionLayer(conv2): sharing filters and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "25-Aug 21:50:36:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "25-Aug 21:50:36:INFO:root:Network constructed!\n",
      "************************************************************\n",
      "          NAME: SVHN-train\n",
      "       BACKEND: CPUBackend\n",
      "  ARCHITECTURE: 14 layers\n",
      "............................................................\n",
      " *** AsyncHDF5DataLayer(train-data)\n",
      "    Outputs ---------------------------\n",
      "          data: Blob(20 x 20 x 1 x 100)\n",
      "         label: Blob(1 x 100)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_in)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 100)\n",
      "............................................................\n",
      " *** ConvolutionLayer(conv1)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 100)\n",
      "    Outputs ---------------------------\n",
      "          conv: Blob(16 x 16 x 96 x 100)\n",
      "............................................................\n",
      " *** PoolingLayer(pool1)\n",
      "    Inputs ----------------------------\n",
      "          conv: Blob(16 x 16 x 96 x 100)\n",
      "    Outputs ---------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 100)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_conv1)\n",
      "    Inputs ----------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 100)\n",
      "............................................................\n",
      " *** ConvolutionLayer(conv2)\n",
      "    Inputs ----------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 100)\n",
      "    Outputs ---------------------------\n",
      "         conv2: Blob(4 x 4 x 128 x 100)\n",
      "............................................................\n",
      " *** PoolingLayer(pool2)\n",
      "    Inputs ----------------------------\n",
      "         conv2: Blob(4 x 4 x 128 x 100)\n",
      "    Outputs ---------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 100)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_conv2)\n",
      "    Inputs ----------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 100)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip1)\n",
      "    Inputs ----------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 100)\n",
      "    Outputs ---------------------------\n",
      "           ip1: Blob(2400 x 100)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_ip1)\n",
      "    Inputs ----------------------------\n",
      "           ip1: Blob(2400 x 100)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip2)\n",
      "    Inputs ----------------------------\n",
      "           ip1: Blob(2400 x 100)\n",
      "    Outputs ---------------------------\n",
      "           ip2: Blob(1200 x 100)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_ip2)\n",
      "    Inputs ----------------------------\n",
      "           ip2: Blob(1200 x 100)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip3)\n",
      "    Inputs ----------------------------\n",
      "           ip2: Blob(1200 x 100)\n",
      "    Outputs ---------------------------\n",
      "           out: Blob(62 x 100)\n",
      "............................................................\n",
      " *** SoftmaxLossLayer(loss)\n",
      "    Inputs ----------------------------\n",
      "           out: Blob(62 x 100)\n",
      "         label: Blob(1 x 100)\n",
      "************************************************************\n",
      "\n",
      "25-Aug 21:50:36:DEBUG:root:Checking network topology for back-propagation\n",
      "25-Aug 21:50:36:INFO:root:Loading existing model from snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0/snapshot-004000.jld\n",
      "25-Aug 21:50:36:DEBUG:root:Loading parameters for layer conv1\n",
      "25-Aug 21:50:36:DEBUG:root:Loading parameters for layer conv2\n",
      "25-Aug 21:50:36:DEBUG:root:Loading parameters for layer ip1\n",
      "25-Aug 21:50:36:DEBUG:root:Loading parameters for layer ip2\n",
      "25-Aug 21:50:36:DEBUG:root:Loading parameters for layer ip3\n",
      "25-Aug 21:50:37:DEBUG:root:Init network SVHN-train\n",
      "25-Aug 21:50:39:DEBUG:root:Initializing coffee breaks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: could not attach metadata for @simd loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-Aug 21:50:39:INFO:root:Merging existing coffee lounge statistics in snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0/statistics.jld\n",
      "25-Aug 21:50:40:DEBUG:root:Init network SVHN-train-prediction\n",
      "25-Aug 21:50:40:DEBUG:root:Init network SVHN-validation-prediction\n",
      "25-Aug 21:50:40:INFO:root:Snapshot directory snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0 already exists\n",
      "25-Aug 21:50:40:INFO:root:ITER = 004000:: TRAIN obj-val = 0.65958869:: LR = 0.00776970:: MOM = 0.95000000\n",
      "25-Aug 21:50:58:INFO:root:\n",
      "25-Aug 21:50:58:INFO:root:## Performance on Validation Set after 4000 iterations\n",
      "25-Aug 21:50:58:INFO:root:---------------------------------------------------------\n",
      "25-Aug 21:50:58:INFO:root:  Accuracy (avg over 6220) = 99.0675%\n",
      "25-Aug 21:50:58:INFO:root:---------------------------------------------------------\n",
      "25-Aug 21:50:58:INFO:root:\n",
      "25-Aug 21:51:01:INFO:root:\n",
      "25-Aug 21:51:01:INFO:root:## Performance on Validation Set after 4000 iterations\n",
      "25-Aug 21:51:01:INFO:root:---------------------------------------------------------\n",
      "25-Aug 21:51:01:INFO:root:  Accuracy (avg over 785) = 70.7006%\n",
      "25-Aug 21:51:01:INFO:root:---------------------------------------------------------\n",
      "25-Aug 21:51:01:INFO:root:\n",
      "25-Aug 21:51:01:INFO:root:Saving snapshot to snapshot-004000.jld...\n",
      "25-Aug 21:51:01:WARNING:root:Overwriting snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0/snapshot-004000.jld...\n",
      "25-Aug 21:51:01:DEBUG:root:Saving parameters for layer conv1\n",
      "25-Aug 21:51:01:DEBUG:root:Saving parameters for layer conv2\n",
      "25-Aug 21:51:01:DEBUG:root:Saving parameters for layer ip1\n",
      "25-Aug 21:51:02:DEBUG:root:Saving parameters for layer ip2\n",
      "25-Aug 21:51:02:DEBUG:root:Saving parameters for layer ip3\n",
      "25-Aug 21:51:05:DEBUG:root:Entering solver loop\n",
      "25-Aug 21:52:32:INFO:root:ITER = 004100:: TRAIN obj-val = 0.40534490:: LR = 0.00772833:: MOM = 0.95000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: could not attach metadata for @simd loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-Aug 21:54:10:INFO:root:ITER = 004200:: TRAIN obj-val = 0.23871490:: LR = 0.00768748:: MOM = 0.95000000\n",
      "25-Aug 21:54:45:INFO:root:Loading existing model from snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0/snapshot-004000.jld\n",
      "25-Aug 21:54:46:DEBUG:root:Loading parameters for layer conv1\n",
      "25-Aug 21:54:46:DEBUG:root:Loading parameters for layer conv2\n",
      "25-Aug 21:54:46:DEBUG:root:Loading parameters for layer ip1\n",
      "25-Aug 21:54:46:DEBUG:root:Loading parameters for layer ip2\n",
      "25-Aug 21:54:46:DEBUG:root:Loading parameters for layer ip3\n",
      "25-Aug 21:54:46:DEBUG:root:Init network SVHN-validation-prediction\n",
      "[nunits_fc1=2400, nunits_fc2=1200, conv1_nfilt=96, conv2_nfilt=128, base_mom=0.95, base_lr=0.01, regu_coef=0.0] => 0.7070063694267515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({:net=>************************************************************\n",
       "          NAME: SVHN-train\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 14 layers\n",
       "............................................................\n",
       " *** AsyncHDF5DataLayer(train-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_in)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "............................................................\n",
       " *** ConvolutionLayer(conv1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "    Outputs ---------------------------\n",
       "          conv: Blob(16 x 16 x 96 x 100)\n",
       "............................................................\n",
       " *** PoolingLayer(pool1)\n",
       "    Inputs ----------------------------\n",
       "          conv: Blob(16 x 16 x 96 x 100)\n",
       "    Outputs ---------------------------\n",
       "          pool: Blob(8 x 8 x 96 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_conv1)\n",
       "    Inputs ----------------------------\n",
       "          pool: Blob(8 x 8 x 96 x 100)\n",
       "............................................................\n",
       " *** ConvolutionLayer(conv2)\n",
       "    Inputs ----------------------------\n",
       "          pool: Blob(8 x 8 x 96 x 100)\n",
       "    Outputs ---------------------------\n",
       "         conv2: Blob(4 x 4 x 128 x 100)\n",
       "............................................................\n",
       " *** PoolingLayer(pool2)\n",
       "    Inputs ----------------------------\n",
       "         conv2: Blob(4 x 4 x 128 x 100)\n",
       "    Outputs ---------------------------\n",
       "         pool2: Blob(2 x 2 x 128 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_conv2)\n",
       "    Inputs ----------------------------\n",
       "         pool2: Blob(2 x 2 x 128 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "         pool2: Blob(2 x 2 x 128 x 100)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(2400 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_ip1)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(2400 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(2400 x 100)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(1200 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(1200 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip3)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(1200 x 100)\n",
       "    Outputs ---------------------------\n",
       "           out: Blob(62 x 100)\n",
       "............................................................\n",
       " *** SoftmaxLossLayer(loss)\n",
       "    Inputs ----------------------------\n",
       "           out: Blob(62 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "************************************************************\n",
       ",:valid_net=>************************************************************\n",
       "          NAME: SVHN-validation-prediction\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 9 layers\n",
       "............................................................\n",
       " *** AsyncHDF5DataLayer(validation-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(20 x 20 x 1 x 785)\n",
       "         label: Blob(1 x 785)\n",
       "............................................................\n",
       " *** ConvolutionLayer(conv1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 785)\n",
       "    Outputs ---------------------------\n",
       "          conv: Blob(16 x 16 x 96 x 785)\n",
       "............................................................\n",
       " *** PoolingLayer(pool1)\n",
       "    Inputs ----------------------------\n",
       "          conv: Blob(16 x 16 x 96 x 785)\n",
       "    Outputs ---------------------------\n",
       "          pool: Blob(8 x 8 x 96 x 785)\n",
       "............................................................\n",
       " *** ConvolutionLayer(conv2)\n",
       "    Inputs ----------------------------\n",
       "          pool: Blob(8 x 8 x 96 x 785)\n",
       "    Outputs ---------------------------\n",
       "         conv2: Blob(4 x 4 x 128 x 785)\n",
       "............................................................\n",
       " *** PoolingLayer(pool2)\n",
       "    Inputs ----------------------------\n",
       "         conv2: Blob(4 x 4 x 128 x 785)\n",
       "    Outputs ---------------------------\n",
       "         pool2: Blob(2 x 2 x 128 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "         pool2: Blob(2 x 2 x 128 x 785)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(2400 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(2400 x 785)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(1200 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip3)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(1200 x 785)\n",
       "    Outputs ---------------------------\n",
       "           out: Blob(62 x 785)\n",
       "............................................................\n",
       " *** AccuracyLayer(validation)\n",
       "    Inputs ----------------------------\n",
       "           out: Blob(62 x 785)\n",
       "         label: Blob(1 x 785)\n",
       "************************************************************\n",
       ",:base_dir=>\"snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0\",:validLabels=>{0,1,5,7,14,19,21,15,24,24  …  21,46,21,59,14,55,22,21,14,40}},(2400,1200,96,128,0.95,0.01,0.0),0.7070063694267515)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model, best_cfg, best_score = gridtune(estfun, evalfun, nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt, base_mom, base_lr, regu_coef; verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-Aug 21:54:51:INFO:root:Constructing net SVHN-Test on CPUBackend...\n",
      "25-Aug 21:54:51:INFO:root:Topological sorting 14 layers...\n",
      "25-Aug 21:54:51:INFO:root:Setup layers...\n",
      "25-Aug 21:54:51:DEBUG:root:ConvolutionLayer(conv1): sharing filters and bias\n",
      "25-Aug 21:54:51:DEBUG:root:ConvolutionLayer(conv2): sharing filters and bias\n",
      "25-Aug 21:54:51:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "25-Aug 21:54:51:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "25-Aug 21:54:51:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "25-Aug 21:54:52:INFO:root:Network constructed!\n",
      "************************************************************\n",
      "          NAME: SVHN-Test\n",
      "       BACKEND: CPUBackend\n",
      "  ARCHITECTURE: 14 layers\n",
      "............................................................\n",
      " *** AsyncHDF5DataLayer(hdf5-data)\n",
      "    Outputs ---------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "         label: Blob(1 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_in)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "............................................................\n",
      " *** ConvolutionLayer(conv1)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "    Outputs ---------------------------\n",
      "          conv: Blob(16 x 16 x 96 x 6220)\n",
      "............................................................\n",
      " *** PoolingLayer(pool1)\n",
      "    Inputs ----------------------------\n",
      "          conv: Blob(16 x 16 x 96 x 6220)\n",
      "    Outputs ---------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_conv1)\n",
      "    Inputs ----------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 6220)\n",
      "............................................................\n",
      " *** ConvolutionLayer(conv2)\n",
      "    Inputs ----------------------------\n",
      "          pool: Blob(8 x 8 x 96 x 6220)\n",
      "    Outputs ---------------------------\n",
      "         conv2: Blob(4 x 4 x 128 x 6220)\n",
      "............................................................\n",
      " *** PoolingLayer(pool2)\n",
      "    Inputs ----------------------------\n",
      "         conv2: Blob(4 x 4 x 128 x 6220)\n",
      "    Outputs ---------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_conv2)\n",
      "    Inputs ----------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip1)\n",
      "    Inputs ----------------------------\n",
      "         pool2: Blob(2 x 2 x 128 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           ip1: Blob(2400 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_ip1)\n",
      "    Inputs ----------------------------\n",
      "           ip1: Blob(2400 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip2)\n",
      "    Inputs ----------------------------\n",
      "           ip1: Blob(2400 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           ip2: Blob(1200 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_ip2)\n",
      "    Inputs ----------------------------\n",
      "           ip2: Blob(1200 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(ip3)\n",
      "    Inputs ----------------------------\n",
      "           ip2: Blob(1200 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           out: Blob(62 x 6220)\n",
      "............................................................\n",
      " *** SoftmaxLayer(prob)\n",
      "    Inputs ----------------------------\n",
      "           out: Blob(62 x 6220)\n",
      "    Outputs ---------------------------\n",
      "          prob: Blob(62 x 6220)\n",
      "************************************************************\n",
      "\n",
      "25-Aug 21:54:52:INFO:root:Loading existing model from snapshot_drop_conv_96_128_2400_1200_0.95_0.01_0.0/snapshot-004000.jld\n",
      "25-Aug 21:54:52:DEBUG:root:Loading parameters for layer conv1\n",
      "25-Aug 21:54:52:DEBUG:root:Loading parameters for layer conv2\n",
      "25-Aug 21:54:52:DEBUG:root:Loading parameters for layer ip1\n",
      "25-Aug 21:54:52:DEBUG:root:Loading parameters for layer ip2\n",
      "25-Aug 21:54:52:DEBUG:root:Loading parameters for layer ip3\n",
      "25-Aug 21:54:52:DEBUG:root:Init network SVHN-Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6220-element Array{Float64,1}:\n",
       "  2.0\n",
       " 19.0\n",
       " 46.0\n",
       " 17.0\n",
       "  2.0\n",
       " 53.0\n",
       " 25.0\n",
       " 10.0\n",
       " 56.0\n",
       " 20.0\n",
       " 14.0\n",
       "  5.0\n",
       " 57.0\n",
       "  ⋮  \n",
       " 17.0\n",
       " 10.0\n",
       "  2.0\n",
       " 11.0\n",
       " 19.0\n",
       "  3.0\n",
       " 14.0\n",
       " 25.0\n",
       " 15.0\n",
       "  4.0\n",
       " 14.0\n",
       " 36.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend = best_model[:net].backend\n",
    "common_layers = best_model[:net].layers[2:end-1]\n",
    "test_batch = size(testImages)[4]\n",
    "\n",
    "test_data_layer = AsyncHDF5DataLayer(source=\"data/test.txt\", batch_size=test_batch, shuffle=false)\n",
    "softmax_layer = SoftmaxLayer(name=\"prob\", tops=[:prob], bottoms=[:out])\n",
    "test_net = Net(\"SVHN-Test\", backend, [test_data_layer, common_layers..., softmax_layer])\n",
    "\n",
    "println(test_net)\n",
    "pred = predict(test_net, best_model[:base_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = joinpath(path, \"julia-Gray-Conv-Dropout-Submission.csv\")\n",
    "dic_rev = Dict(zip(values(dic), keys(dic)))\n",
    "labelsInfoTest[:Class] = map(k -> dic_rev[k], pred)\n",
    "writetable(filename, labelsInfoTest[:,[:ID, :Class]], separator=',', header=true)\n",
    "run(`sed -i '' 's/\"//g' $(filename)`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
