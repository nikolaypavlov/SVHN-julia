{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Colors\n",
    "using Images\n",
    "using DataFrames\n",
    "using HDF5\n",
    "using MLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageSize = (20,20,1) # 20 x 20 pixel x 1 color\n",
    "\n",
    "#Set location of data files, folders\n",
    "path = \"data\"\n",
    "\n",
    "#Read information about training data , IDs.\n",
    "labelsInfo = readtable(\"$(path)/trainLabels.csv\")\n",
    "\n",
    "println(\"Size of data set: \", size(labelsInfo))\n",
    "\n",
    "labelsInfoTest = readtable(\"$(path)/sampleSubmission.csv\")\n",
    "\n",
    "println(\"Size of test data set: \", size(labelsInfoTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need labels from 0 to N-1 for Mocha\n",
    "labs = unique(labelsInfo[:Class])\n",
    "dic = Dict(zip(collect(labs), 0:length(labs)-1))\n",
    "create_labs(classes) = map(k -> dic[k], classes)\n",
    "labelsInfo[:Labels] = create_labs(labelsInfo[:Class])\n",
    "labelsInfoTest[:Labels] = create_labs(labelsInfoTest[:Class])\n",
    "head(labelsInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on train and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(12345)\n",
    "n = length(labelsInfo[:ID])\n",
    "trainSet = shuffle(1:n .> n*0.25)\n",
    "labelsInfoTrain = labelsInfo[trainSet,:]\n",
    "\n",
    "# Hold out some data from validation set\n",
    "srand(12345)\n",
    "n = length(labelsInfo[!trainSet,:ID])\n",
    "validationSet = shuffle(1:n .> n*0.5)\n",
    "labelsInfoValid = labelsInfo[!trainSet,:][validationSet,:]\n",
    "labelsInfoHoldout = labelsInfo[!trainSet,:][!validationSet,:]\n",
    "\n",
    "println(\"Size of the train data set: \", size(labelsInfoTrain))\n",
    "println(\"Size of the validation data set: \", size(labelsInfoValid))\n",
    "println(\"Size of the holdout data set: \", size(labelsInfoHoldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images from the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function readImages(typeData, labelsInfo, imageSize, path)\n",
    "    w, h, c = imageSize\n",
    "    n = length(labelsInfo[:ID])\n",
    "    x = float32(zeros(w,h,c,n))\n",
    "    for (index, idImage) in enumerate(labelsInfo[:ID]) \n",
    "        #Read image file \n",
    "        nameFile = \"$(path)/$(typeData)Resized/$(idImage).Bmp\"\n",
    "        img = imread(nameFile)\n",
    "\n",
    "        #Convert img to float values \n",
    "        img = convert(Array{Gray}, img)\n",
    "        img = convert(Array{Float32}, img)\n",
    "        #img = separate(convert(Image{RGB}, img))\n",
    "        #img = convert(Array, img)\n",
    "        #img = img[:,:,1:3]\n",
    "        \n",
    "        # Normalize data\n",
    "        img = (img - mean(img)) / std(img)\n",
    "        x[:,:,:,index] = reshape(img, w, h, c)\n",
    "        \n",
    "    end \n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainImages = readImages(\"train\", labelsInfoTrain, imageSize, path)\n",
    "println(\"Size of the train images set: \", size(trainImages))\n",
    "\n",
    "validImages = readImages(\"train\", labelsInfoValid, imageSize, path)\n",
    "println(\"Size of the validation images set: \", size(validImages))\n",
    "\n",
    "holdoutImages = readImages(\"train\", labelsInfoHoldout, imageSize, path)\n",
    "println(\"Size of the holdout images set: \", size(validImages))\n",
    "\n",
    "testImages = readImages(\"test\", labelsInfoTest, imageSize, path)\n",
    "println(\"Size of the test images set: \", size(testImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images into HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data to HDF5 format\n",
    "function convert_to_HDF5(typeData, imageSet, labelsInfo, path)\n",
    "    w, h, c, n = size(imageSet)\n",
    "    \n",
    "    HDF5.h5open(\"$(path)/$(typeData).hdf5\", \"w\") do h5\n",
    "        dset_data = d_create(h5, \"data\", datatype(Float32), dataspace(w, h, c, n))\n",
    "        dset_data[:,:,:,:] =  imageSet\n",
    "        \n",
    "        dset_label = d_create(h5, \"label\", datatype(Float32), dataspace(1,n))\n",
    "        dset_label[1,:] = labelsInfo[:Labels]\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "convert_to_HDF5(\"train\", trainImages, labelsInfoTrain, path)\n",
    "convert_to_HDF5(\"validation\", validImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"holdout\", holdoutImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"test\", testImages, labelsInfoTest, path)\n",
    "\n",
    "run(`echo $(path)/train.hdf5` |> \"$(path)/train.txt\")\n",
    "run(`echo $(path)/validation.hdf5` |> \"$(path)/validation.txt\")\n",
    "run(`echo $(path)/holdout.hdf5` |> \"$(path)/holdout.txt\")\n",
    "run(`echo $(path)/test.hdf5` |> \"$(path)/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_gpu = false\n",
    "use_native = false\n",
    "\n",
    "if use_gpu\n",
    "    ENV[\"MOCHA_USE_CUDA\"] = \"true\"\n",
    "elseif use_native\n",
    "    ENV[\"MOCHA_USE_NATIVE_EXT\"] = \"true\"\n",
    "    ENV[\"OMP_NUM_THREADS\"] = 6\n",
    "else \n",
    "    blas_set_num_threads(6) \n",
    "end\n",
    "\n",
    "using Mocha\n",
    "srand(333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH = 100\n",
    "EPOCH = int(round(size(trainImages)[4] / TRAIN_BATCH))\n",
    "MAXITER = 180*EPOCH\n",
    "\n",
    "NCLASSES = length(unique(labelsInfoTrain[:Class]))\n",
    "nunits_fc1 = (\"nunits_fc1\", [2400])\n",
    "nunits_fc2 = (\"nunits_fc2\", [1200])\n",
    "base_mom = (\"base_mom\", [0.95])\n",
    "base_lr = (\"base_lr\", [0.01])\n",
    "regu_coef = (\"regu_coef\", [0.0])\n",
    "conv1_nfilt = (\"conv1_nfilt\", [96])\n",
    "conv2_nfilt = (\"conv2_nfilt\", [128])\n",
    "\n",
    "n = length(nunits_fc1[2]) * length(nunits_fc2[2]) * length(base_mom[2]) * length(base_lr[2]) * length(regu_coef[2]) * length(conv1_nfilt[2]) * length(conv2_nfilt[2])\n",
    "println(\"Grid search will run \", n, \" times\")\n",
    "println(\"Solver will run for \", MAXITER, \" iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Neural Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function configure_training(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt)\n",
    "    data_layer  = AsyncHDF5DataLayer(name=\"train-data\", source=\"$(path)/train.txt\", \n",
    "                                    batch_size=TRAIN_BATCH)\n",
    "    conv_layer  = ConvolutionLayer(name=\"conv1\", n_filter=conv1_nfilt, kernel=(5,5), bottoms=[:data], tops=[:conv])\n",
    "    pool_layer  = PoolingLayer(name=\"pool1\", kernel=(2,2), stride=(2,2), bottoms=[:conv], tops=[:pool])\n",
    "    conv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=conv2_nfilt, kernel=(5,5), bottoms=[:pool], tops=[:conv2])\n",
    "    pool2_layer = PoolingLayer(name=\"pool2\", kernel=(2,2), stride=(2,2), bottoms=[:conv2], tops=[:pool2])\n",
    "    fc1_layer   = InnerProductLayer(name=\"ip1\", output_dim=nunits_fc1, neuron=Neurons.ReLU(), bottoms=[:pool2], tops=[:ip1])\n",
    "    fc2_layer   = InnerProductLayer(name=\"ip2\", output_dim=nunits_fc2, neuron=Neurons.ReLU(), bottoms=[:ip1], tops=[:ip2])\n",
    "    fc3_layer   = InnerProductLayer(name=\"ip3\", output_dim=NCLASSES, bottoms=[:ip2], tops=[:out])\n",
    "    loss_layer  = SoftmaxLossLayer(name=\"loss\", bottoms=[:out,:label])\n",
    "    \n",
    "    backend = use_gpu ? GPUBackend() : CPUBackend()\n",
    "    init(backend)\n",
    "\n",
    "    # setup dropout for the different layers\n",
    "    # we use 20% dropout on the inputs and 50% dropout in the hidden layers\n",
    "    # as these values were previously found to be good defaults\n",
    "    drop_input = DropoutLayer(name=\"drop_in\", bottoms=[:data], ratio=0.1)\n",
    "    drop_conv1 = DropoutLayer(name=\"drop_conv1\", bottoms=[:pool], ratio=0.2)\n",
    "    drop_conv2 = DropoutLayer(name=\"drop_conv2\", bottoms=[:pool2], ratio=0.2)\n",
    "    drop_ip1 = DropoutLayer(name=\"drop_ip1\", bottoms=[:ip1], ratio=0.5)\n",
    "    drop_ip2= DropoutLayer(name=\"drop_ip2\", bottoms=[:ip2], ratio=0.5)\n",
    "\n",
    "    common_layers = [conv_layer, pool_layer, conv2_layer, pool2_layer, fc1_layer, fc2_layer, fc3_layer]\n",
    "    drop_layers = [drop_input, drop_conv1, drop_conv2, drop_ip1, drop_ip2]\n",
    "    # put training net together, note that the correct ordering will automatically be established by the constructor\n",
    "    net = Net(\"SVHN-train\", backend, [data_layer, common_layers..., drop_layers..., loss_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    full_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"train-full-data\", \n",
    "        source=\"$(path)/train.txt\", \n",
    "        batch_size=size(testImages)[4])\n",
    "    full_acc_layer = AccuracyLayer(name=\"full_train\", bottoms=[:out, :label], report_error=true)\n",
    "    train_net = Net(\"SVHN-train-prediction\", backend, [full_data_layer, common_layers..., full_acc_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    valid_batch = size(validImages)[4]\n",
    "    valid_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"validation-data\", \n",
    "        source=\"$(path)/validation.txt\", \n",
    "        batch_size=valid_batch)\n",
    "    valid_acc_layer = AccuracyLayer(name=\"validation\", bottoms=[:out, :label], report_error=true)\n",
    "    valid_net = Net(\"SVHN-validation-prediction\", backend, [valid_data_layer, common_layers..., valid_acc_layer])\n",
    "    \n",
    "    println(net)\n",
    "    #println(valid_net)\n",
    "    return(net, train_net, valid_net, common_layers) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function configure_solver(niter, base_mom, base_lr, epoch, base_dir, regu_coef=0.0)\n",
    "    params = SolverParameters(\n",
    "                max_iter=niter,\n",
    "                regu_coef=regu_coef,\n",
    "                mom_policy=MomPolicy.Fixed(base_mom),\n",
    "                lr_policy=LRPolicy.Inv(base_lr, 0.0001, 0.75), \n",
    "                load_from=base_dir)\n",
    "\n",
    "    solver = SGD(params)\n",
    "    \n",
    "    return(solver)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup coffee breaks for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function configure_coffebreaks(solver, train_net, valid_net, base_dir)\n",
    "    setup_coffee_lounge(solver, save_into=\"$(base_dir)/statistics.jld\", every_n_iter=5000)\n",
    "\n",
    "    # report training progress every 100 iterations\n",
    "    add_coffee_break(solver, TrainingSummary(show_lr=true, show_mom=true), every_n_iter=100)\n",
    "\n",
    "    # Report train perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(train_net), every_n_iter=500)\n",
    "    \n",
    "    # Report validation perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=500)\n",
    "    \n",
    "    # save snapshots every 1000 iterations\n",
    "    add_coffee_break(solver, Snapshot(base_dir), every_n_iter=1000) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure grid serach params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function predict(predict_net::Net, base_dir::String) \n",
    "    load_snapshot(predict_net, base_dir)\n",
    "    \n",
    "    init(predict_net)\n",
    "    forward_epoch(predict_net)\n",
    "\n",
    "    batch = []\n",
    "    if isa(predict_net.states[end].layer, AccuracyLayer)\n",
    "        batch = to_array(predict_net.states[end-1].blobs[1])\n",
    "    else \n",
    "        batch = to_array(predict_net.states[end].blobs[1])\n",
    "    end\n",
    "    \n",
    "    n = size(batch)[2]\n",
    "    pred = zeros(n)\n",
    "    for i in 1:n\n",
    "        pred[i] = indmax(batch[:,i]) - 1\n",
    "    end\n",
    "    \n",
    "    return(pred)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function evalfun(netInfo)\n",
    "    pred = predict(netInfo[:valid_net], netInfo[:base_dir])\n",
    "    model_perfomance = mean(pred .== netInfo[:validLabels])\n",
    "    \n",
    "    return(model_perfomance)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function estfun(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt, base_mom, base_lr, regu_coef)\n",
    "    snapshot_dir = \"snapshot_drop_conv_$(conv1_nfilt)_$(conv2_nfilt)_$(nunits_fc1)_$(nunits_fc2)_$(base_mom)_$(base_lr)_$(regu_coef)\"\n",
    "    net, train_net, valid_net, common_layers = configure_training(nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt)\n",
    "    solver = configure_solver(MAXITER, base_mom, base_lr, EPOCH, snapshot_dir, regu_coef)\n",
    "    configure_coffebreaks(solver, train_net, valid_net, snapshot_dir)\n",
    "    solve(solver, net) \n",
    "    model = {:net => net, \n",
    "             :valid_net => valid_net,\n",
    "             :base_dir => snapshot_dir, \n",
    "             :validLabels => labelsInfoValid[:Labels],\n",
    "             :common_layers => common_layers}\n",
    "    \n",
    "    return(model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model, best_cfg, best_score = gridtune(estfun, evalfun, nunits_fc1, nunits_fc2, conv1_nfilt, conv2_nfilt, base_mom, base_lr, regu_coef; verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backend = best_model[:net].backend\n",
    "common_layers = best_model[:common_layers]\n",
    "test_batch = size(testImages)[4]\n",
    "\n",
    "test_data_layer = AsyncHDF5DataLayer(source=\"data/test.txt\", batch_size=test_batch, shuffle=false)\n",
    "softmax_layer = SoftmaxLayer(name=\"prob\", tops=[:prob], bottoms=[:out])\n",
    "test_net = Net(\"SVHN-Test\", backend, [test_data_layer, common_layers..., softmax_layer])\n",
    "\n",
    "println(test_net)\n",
    "pred = predict(test_net, best_model[:base_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = joinpath(path, \"julia-Gray-Conv-Dropout-Submission.csv\")\n",
    "dic_rev = Dict(zip(values(dic), keys(dic)))\n",
    "labelsInfoTest[:Class] = map(k -> dic_rev[k], pred)\n",
    "writetable(filename, labelsInfoTest[:,[:ID, :Class]], separator=',', header=true)\n",
    "run(`sed --in-place='' 's/\"//g' $(filename)`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
