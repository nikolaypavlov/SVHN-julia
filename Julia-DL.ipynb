{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Images\n",
    "using DataFrames\n",
    "using HDF5\n",
    "using MLBase\n",
    "\n",
    "use_gpu = false\n",
    "if use_gpu\n",
    "    ENV[\"MOCHA_USE_CUDA\"] = \"true\"\n",
    "else\n",
    "    ENV[\"MOCHA_USE_NATIVE_EXT\"] = \"true\"\n",
    "    blas_set_num_threads(2)\n",
    "end\n",
    "\n",
    "using Mocha\n",
    "srand(333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set: (6283,2)\n",
      "Size of test data set: (6220,2)\n"
     ]
    }
   ],
   "source": [
    "imageSize = (20,20,1) # 20 x 20 pixel x 1 color\n",
    "\n",
    "#Set location of data files, folders\n",
    "path = \"data\"\n",
    "\n",
    "#Read information about training data , IDs.\n",
    "labelsInfo = readtable(\"$(path)/trainLabels.csv\")\n",
    "\n",
    "println(\"Size of data set: \", size(labelsInfo))\n",
    "\n",
    "labelsInfoTest = readtable(\"$(path)/sampleSubmission.csv\")\n",
    "\n",
    "println(\"Size of test data set: \", size(labelsInfoTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><tr><th></th><th>ID</th><th>Class</th><th>Labels</th></tr><tr><th>1</th><td>1</td><td>n</td><td>0</td></tr><tr><th>2</th><td>2</td><td>8</td><td>1</td></tr><tr><th>3</th><td>3</td><td>T</td><td>2</td></tr><tr><th>4</th><td>4</td><td>I</td><td>3</td></tr><tr><th>5</th><td>5</td><td>R</td><td>4</td></tr><tr><th>6</th><td>6</td><td>W</td><td>5</td></tr></table>"
      ],
      "text/plain": [
       "6x3 DataFrame\n",
       "| Row | ID | Class | Labels |\n",
       "|-----|----|-------|--------|\n",
       "| 1   | 1  | \"n\"   | 0      |\n",
       "| 2   | 2  | \"8\"   | 1      |\n",
       "| 3   | 3  | \"T\"   | 2      |\n",
       "| 4   | 4  | \"I\"   | 3      |\n",
       "| 5   | 5  | \"R\"   | 4      |\n",
       "| 6   | 6  | \"W\"   | 5      |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need labels from 0 to N-1 for Mocha\n",
    "labs = unique(labelsInfo[:Class])\n",
    "dic = Dict(zip(collect(labs), 0:length(labs)-1))\n",
    "create_labs(classes) = map(k -> dic[k], classes)\n",
    "labelsInfo[:Labels] = create_labs(labelsInfo[:Class])\n",
    "labelsInfoTest[:Labels] = create_labs(labelsInfoTest[:Class])\n",
    "head(labelsInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on train and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train data set: (4713,3)\n",
      "Size of the validation data set: (785,3)\n",
      "Size of the holdout data set: (785,3)\n"
     ]
    }
   ],
   "source": [
    "srand(12345)\n",
    "n = length(labelsInfo[:ID])\n",
    "trainSet = shuffle(1:n .> n*0.25)\n",
    "labelsInfoTrain = labelsInfo[trainSet,:]\n",
    "\n",
    "# Hold out some data from validation set\n",
    "srand(12345)\n",
    "n = length(labelsInfo[!trainSet,:ID])\n",
    "validationSet = shuffle(1:n .> n*0.5)\n",
    "labelsInfoValid = labelsInfo[!trainSet,:][validationSet,:]\n",
    "labelsInfoHoldout = labelsInfo[!trainSet,:][!validationSet,:]\n",
    "\n",
    "println(\"Size of the train data set: \", size(labelsInfoTrain))\n",
    "println(\"Size of the validation data set: \", size(labelsInfoValid))\n",
    "println(\"Size of the holdout data set: \", size(labelsInfoHoldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images from the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readImages (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readImages(typeData, labelsInfo, imageSize, path)\n",
    "    w, h, c = imageSize\n",
    "    n = length(labelsInfo[:ID])\n",
    "    x = float32(zeros(w,h,c,n))\n",
    "    for (index, idImage) in enumerate(labelsInfo[:ID]) \n",
    "        #Read image file \n",
    "        nameFile = \"$(path)/$(typeData)Resized/$(idImage).Bmp\"\n",
    "        img = imread(nameFile)\n",
    "\n",
    "        #Convert img to float values \n",
    "        img = convert(Array{Gray}, img)\n",
    "        img = convert(Array{Float32}, img)\n",
    "        \n",
    "        # Normalize data\n",
    "        img = (img - mean(img)) / std(img)\n",
    "        x[:,:,:,index] = reshape(img, w, h, c)\n",
    "        \n",
    "    end \n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train images set: (20,20,1,4713)\n",
      "Size of the validation images set: (20,20,1,785)\n",
      "Size of the holdout images set: (20,20,1,785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OSX reader: indexed color images not implemented\n",
      "WARNING: OSX reader: indexed color images not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the test images set: (20,20,1,6220)\n"
     ]
    }
   ],
   "source": [
    "trainImages = readImages(\"train\", labelsInfoTrain, imageSize, path)\n",
    "println(\"Size of the train images set: \", size(trainImages))\n",
    "\n",
    "validImages = readImages(\"train\", labelsInfoValid, imageSize, path)\n",
    "println(\"Size of the validation images set: \", size(validImages))\n",
    "\n",
    "holdoutImages = readImages(\"train\", labelsInfoHoldout, imageSize, path)\n",
    "println(\"Size of the holdout images set: \", size(validImages))\n",
    "\n",
    "testImages = readImages(\"test\", labelsInfoTest, imageSize, path)\n",
    "println(\"Size of the test images set: \", size(testImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images into HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data to HDF5 format\n",
    "function convert_to_HDF5(typeData, imageSet, labelsInfo, path)\n",
    "    w, h, c, n = size(imageSet)\n",
    "    \n",
    "    HDF5.h5open(\"$(path)/$(typeData).hdf5\", \"w\") do h5\n",
    "        dset_data = d_create(h5, \"data\", datatype(Float32), dataspace(w, h, c, n))\n",
    "        dset_data[:,:,:,:] =  imageSet\n",
    "        \n",
    "        dset_label = d_create(h5, \"label\", datatype(Float32), dataspace(1,n))\n",
    "        dset_label[1,:] = labelsInfo[:Labels]\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "convert_to_HDF5(\"train\", trainImages, labelsInfoTrain, path)\n",
    "convert_to_HDF5(\"validation\", validImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"holdout\", holdoutImages, labelsInfoValid, path)\n",
    "convert_to_HDF5(\"test\", testImages, labelsInfoTest, path)\n",
    "\n",
    "run(`echo $(path)/train.hdf5` |> \"$(path)/train.txt\")\n",
    "run(`echo $(path)/validation.hdf5` |> \"$(path)/validation.txt\")\n",
    "run(`echo $(path)/holdout.hdf5` |> \"$(path)/holdout.txt\")\n",
    "run(`echo $(path)/test.hdf5` |> \"$(path)/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search will run 1 times\n",
      "Solver will run for 2585 iterations\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH = 100\n",
    "EPOCH = int(round(size(trainImages)[4] / TRAIN_BATCH))\n",
    "MAXITER = 55*EPOCH\n",
    "\n",
    "nclasses = length(unique(labelsInfoTrain[:Class]))\n",
    "nunits_l1 = (\"nunits_l1\", [2400])\n",
    "nunits_l2 = (\"nunits_l2\", [1200])\n",
    "base_mom = (\"base_mom\", [0.1])\n",
    "base_lr = (\"base_lr\", [0.3])\n",
    "\n",
    "n = length(nunits_l1[2]) * length(nunits_l2[2]) * length(base_mom[2]) * length(base_lr[2])\n",
    "println(\"Grid search will run \", n, \" times\")\n",
    "println(\"Solver will run for \", MAXITER, \" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(predict_net::Net, base_dir::String) \n",
    "    load_snapshot(predict_net, base_dir)\n",
    "    \n",
    "    init(predict_net)\n",
    "    forward_epoch(predict_net)\n",
    "\n",
    "    batch = []\n",
    "    if isa(predict_net.states[end].layer, AccuracyLayer)\n",
    "        batch = to_array(predict_net.states[end-1].blobs[1])\n",
    "    else \n",
    "        batch = to_array(predict_net.states[end].blobs[1])\n",
    "    end\n",
    "    \n",
    "    n = size(batch)[2]\n",
    "    pred = zeros(n)\n",
    "    for i in 1:n\n",
    "        pred[i] = indmax(batch[:,i]) - 1\n",
    "    end\n",
    "    \n",
    "    return(pred)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Neural Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_training (generic function with 2 methods)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_training(nunits_l1, nunits_l2)\n",
    "    data_layer  = AsyncHDF5DataLayer(name=\"train-data\", source=\"$(path)/train.txt\", \n",
    "                                    batch_size=TRAIN_BATCH)\n",
    "    fc1_layer   = InnerProductLayer(name=\"fc1\", output_dim=nunits_l1, neuron=Neurons.ReLU(),\n",
    "                                    weight_init = GaussianInitializer(std=0.01),\n",
    "                                    weight_cons = L2Cons(4),\n",
    "                                    bottoms=[:data], tops=[:fc1])\n",
    "    fc2_layer   = InnerProductLayer(name=\"fc2\", output_dim=nunits_l2, neuron=Neurons.ReLU(),\n",
    "                                    weight_init = GaussianInitializer(std=0.01),\n",
    "                                    weight_cons = L2Cons(4),\n",
    "                                    bottoms=[:fc1], tops=[:fc2])\n",
    "    fc3_layer   = InnerProductLayer(name=\"out\", output_dim=nclasses, bottoms=[:fc2],\n",
    "                                    weight_init = ConstantInitializer(0),\n",
    "                                    weight_cons = L2Cons(4),\n",
    "                                    tops=[:out])\n",
    "    loss_layer  = SoftmaxLossLayer(name=\"loss\", bottoms=[:out,:label])\n",
    "    \n",
    "    backend = use_gpu ? GPUBackend() : CPUBackend()\n",
    "    init(backend)\n",
    "\n",
    "    # setup dropout for the different layers\n",
    "    # we use 20% dropout on the inputs and 50% dropout in the hidden layers\n",
    "    # as these values were previously found to be good defaults\n",
    "    drop_input  = DropoutLayer(name=\"drop_in\", bottoms=[:data], ratio=0.1)\n",
    "    drop_fc1 = DropoutLayer(name=\"drop_fc1\", bottoms=[:fc1], ratio=0.5)\n",
    "    #drop_fc2  = DropoutLayer(name=\"drop_fc2\", bottoms=[:fc2], ratio=0.5)\n",
    "\n",
    "    common_layers = [fc1_layer, fc2_layer, fc3_layer]\n",
    "    drop_layers = [drop_input, drop_fc1]\n",
    "    # put training net together, note that the correct ordering will automatically be established by the constructor\n",
    "    net = Net(\"SVHN-train\", backend, [data_layer, common_layers..., drop_layers..., loss_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    full_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"train-full-data\", \n",
    "        source=\"$(path)/train.txt\", \n",
    "        batch_size=size(testImages)[4])\n",
    "    full_acc_layer = AccuracyLayer(name=\"full_train\", bottoms=[:out, :label], report_error=true)\n",
    "    train_net = Net(\"SVHN-train-prediction\", backend, [full_data_layer, common_layers..., full_acc_layer])\n",
    "    \n",
    "    # Configure accuracy check on validation set during training process\n",
    "    valid_batch = size(validImages)[4]\n",
    "    valid_data_layer = AsyncHDF5DataLayer(\n",
    "        name=\"validation-data\", \n",
    "        source=\"$(path)/validation.txt\", \n",
    "        batch_size=valid_batch)\n",
    "    valid_acc_layer = AccuracyLayer(name=\"validation\", bottoms=[:out, :label], report_error=true)\n",
    "    valid_net = Net(\"SVHN-validation-prediction\", backend, [valid_data_layer, common_layers..., valid_acc_layer])\n",
    "    \n",
    "    #println(net)\n",
    "    #println(valid_net)\n",
    "    return(net, train_net, valid_net) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_solver(niter, base_mom, base_lr, epoch, base_dir, regu_coef=0.00)\n",
    "    params = SolverParameters(\n",
    "                max_iter=niter,\n",
    "                regu_coef=0.00,\n",
    "                mom_policy=MomPolicy.Linear(base_mom, 0.0008, epoch, 0.99),\n",
    "                lr_policy=LRPolicy.Step(base_lr, 0.998, epoch), \n",
    "                load_from=base_dir)\n",
    "\n",
    "    solver = SGD(params)\n",
    "    \n",
    "    return(solver)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup coffee breaks for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configure_coffebreaks (generic function with 2 methods)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function configure_coffebreaks(solver, train_net, valid_net, base_dir)\n",
    "    setup_coffee_lounge(solver, save_into=\"$(base_dir)/statistics.jld\", every_n_iter=5000)\n",
    "\n",
    "    # report training progress every 100 iterations\n",
    "    add_coffee_break(solver, TrainingSummary(show_lr=true, show_mom=true), every_n_iter=100)\n",
    "\n",
    "    # Report train perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(train_net), every_n_iter=500)\n",
    "    \n",
    "    # Report validation perfomance every 500 iterations\n",
    "    add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=500)\n",
    "    \n",
    "    # save snapshots every 1000 iterations\n",
    "    add_coffee_break(solver, Snapshot(base_dir), every_n_iter=1000) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure grid serach params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalfun (generic function with 1 method)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evalfun(netInfo)\n",
    "    pred = predict(netInfo[:valid_net], netInfo[:base_dir])\n",
    "    model_perfomance = mean(pred .== netInfo[:validLabels])\n",
    "    \n",
    "    return(model_perfomance)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estfun (generic function with 2 methods)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function estfun(nunits_l1, nunits_l2, base_mom, base_lr)\n",
    "    snapshot_dir = \"snapshot_mlp_dropout_$(nunits_l1)_$(nunits_l2)_$(base_mom)_$(base_lr)\"\n",
    "    net, train_net, valid_net = configure_training(nunits_l1, nunits_l2)\n",
    "    solver = configure_solver(MAXITER, base_mom, base_lr, EPOCH, snapshot_dir)\n",
    "    configure_coffebreaks(solver, train_net, valid_net, snapshot_dir)\n",
    "    solve(solver, net) \n",
    "    model = {:net => net, \n",
    "             :valid_net => valid_net,\n",
    "             :base_dir => snapshot_dir, \n",
    "             :validLabels => labelsInfoValid[:Labels]}\n",
    "    \n",
    "    return(model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21-Aug 12:58:47:INFO:root:Constructing net SVHN-train on CPUBackend...\n",
      "21-Aug 12:58:47:INFO:root:Topological sorting 7 layers...\n",
      "21-Aug 12:58:47:INFO:root:Setup layers...\n",
      "21-Aug 12:58:48:INFO:root:Network constructed!\n",
      "21-Aug 12:58:48:INFO:root:Constructing net SVHN-train-prediction on CPUBackend...\n",
      "21-Aug 12:58:48:INFO:root:Topological sorting 5 layers...\n",
      "21-Aug 12:58:48:INFO:root:Setup layers...\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(fc1): sharing weights and bias\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(fc2): sharing weights and bias\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(out): sharing weights and bias\n",
      "21-Aug 12:58:48:INFO:root:Network constructed!\n",
      "21-Aug 12:58:48:INFO:root:Constructing net SVHN-validation-prediction on CPUBackend...\n",
      "21-Aug 12:58:48:INFO:root:Topological sorting 5 layers...\n",
      "21-Aug 12:58:48:INFO:root:Setup layers...\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(fc1): sharing weights and bias\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(fc2): sharing weights and bias\n",
      "21-Aug 12:58:48:DEBUG:root:InnerProductLayer(out): sharing weights and bias\n",
      "21-Aug 12:58:48:INFO:root:Network constructed!\n",
      "21-Aug 12:58:48:DEBUG:root:Checking network topology for back-propagation\n",
      "21-Aug 12:58:48:INFO:root:Loading existing model from snapshot_mlp_dropout_2400_1200_0.1_0.3/snapshot-002000.jld\n",
      "21-Aug 12:58:48:DEBUG:root:Loading parameters for layer fc1\n",
      "21-Aug 12:58:48:DEBUG:root:Loading parameters for layer fc2\n",
      "21-Aug 12:58:48:DEBUG:root:Loading parameters for layer out\n",
      "21-Aug 12:58:48:DEBUG:root:Init network SVHN-train\n",
      "21-Aug 12:58:48:DEBUG:root:Initializing coffee breaks\n",
      "21-Aug 12:58:48:INFO:root:Merging existing coffee lounge statistics in snapshot_mlp_dropout_2400_1200_0.1_0.3/statistics.jld\n",
      "21-Aug 12:58:48:DEBUG:root:Init network SVHN-train-prediction\n",
      "21-Aug 12:58:48:DEBUG:root:Init network SVHN-validation-prediction\n",
      "21-Aug 12:58:48:INFO:root:Snapshot directory snapshot_mlp_dropout_2400_1200_0.1_0.3 already exists\n",
      "21-Aug 12:58:48:INFO:root:ITER = 002000:: TRAIN obj-val = 0.12142621:: LR = 0.27580618:: MOM = 0.13360000\n",
      "21-Aug 12:59:01:INFO:root:\n",
      "21-Aug 12:59:01:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "21-Aug 12:59:01:INFO:root:---------------------------------------------------------\n",
      "21-Aug 12:59:01:INFO:root:  Accuracy (avg over 6220) = 99.2605%\n",
      "21-Aug 12:59:01:INFO:root:---------------------------------------------------------\n",
      "21-Aug 12:59:01:INFO:root:\n",
      "21-Aug 12:59:03:INFO:root:\n",
      "21-Aug 12:59:03:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "21-Aug 12:59:03:INFO:root:---------------------------------------------------------\n",
      "21-Aug 12:59:03:INFO:root:  Accuracy (avg over 785) = 65.8599%\n",
      "21-Aug 12:59:03:INFO:root:---------------------------------------------------------\n",
      "21-Aug 12:59:03:INFO:root:\n",
      "21-Aug 12:59:03:INFO:root:Saving snapshot to snapshot-002000.jld...\n",
      "21-Aug 12:59:03:WARNING:root:Overwriting snapshot_mlp_dropout_2400_1200_0.1_0.3/snapshot-002000.jld...\n",
      "21-Aug 12:59:03:DEBUG:root:Saving parameters for layer fc1\n",
      "21-Aug 12:59:03:DEBUG:root:Saving parameters for layer fc2\n",
      "21-Aug 12:59:03:DEBUG:root:Saving parameters for layer out\n",
      "21-Aug 12:59:04:DEBUG:root:Entering solver loop\n",
      "21-Aug 13:00:06:INFO:root:ITER = 002100:: TRAIN obj-val = 0.03261026:: LR = 0.27470406:: MOM = 0.13520000\n",
      "21-Aug 13:01:01:INFO:root:ITER = 002200:: TRAIN obj-val = 0.08229424:: LR = 0.27360634:: MOM = 0.13680000\n",
      "21-Aug 13:01:57:INFO:root:ITER = 002300:: TRAIN obj-val = 0.02482260:: LR = 0.27251301:: MOM = 0.13840000\n",
      "21-Aug 13:02:56:INFO:root:ITER = 002400:: TRAIN obj-val = 0.06886493:: LR = 0.27088120:: MOM = 0.14080000\n",
      "21-Aug 13:04:09:INFO:root:ITER = 002500:: TRAIN obj-val = 0.05892942:: LR = 0.26979876:: MOM = 0.14240000\n",
      "21-Aug 13:04:23:INFO:root:\n",
      "21-Aug 13:04:23:INFO:root:## Performance on Validation Set after 2500 iterations\n",
      "21-Aug 13:04:23:INFO:root:---------------------------------------------------------\n",
      "21-Aug 13:04:23:INFO:root:  Accuracy (avg over 6220) = 99.7267%\n",
      "21-Aug 13:04:23:INFO:root:---------------------------------------------------------\n",
      "21-Aug 13:04:23:INFO:root:\n",
      "21-Aug 13:04:25:INFO:root:\n",
      "21-Aug 13:04:25:INFO:root:## Performance on Validation Set after 2500 iterations\n",
      "21-Aug 13:04:25:INFO:root:---------------------------------------------------------\n",
      "21-Aug 13:04:25:INFO:root:  Accuracy (avg over 785) = 65.3503%\n",
      "21-Aug 13:04:25:INFO:root:---------------------------------------------------------\n",
      "21-Aug 13:04:25:INFO:root:\n",
      "21-Aug 13:05:25:INFO:root:Loading existing model from snapshot_mlp_dropout_2400_1200_0.1_0.3/snapshot-002000.jld\n",
      "21-Aug 13:05:26:DEBUG:root:Loading parameters for layer fc1\n",
      "21-Aug 13:05:26:DEBUG:root:Loading parameters for layer fc2\n",
      "21-Aug 13:05:26:DEBUG:root:Loading parameters for layer out\n",
      "21-Aug 13:05:26:DEBUG:root:Init network SVHN-validation-prediction\n",
      "[nunits_l1=2400, nunits_l2=1200, base_mom=0.1, base_lr=0.3] => 0.6585987261146496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({:net=>************************************************************\n",
       "          NAME: SVHN-train\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 7 layers\n",
       "............................................................\n",
       " *** AsyncHDF5DataLayer(train-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_in)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(fc1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 100)\n",
       "    Outputs ---------------------------\n",
       "           fc1: Blob(2400 x 100)\n",
       "............................................................\n",
       " *** DropoutLayer(drop_fc1)\n",
       "    Inputs ----------------------------\n",
       "           fc1: Blob(2400 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(fc2)\n",
       "    Inputs ----------------------------\n",
       "           fc1: Blob(2400 x 100)\n",
       "    Outputs ---------------------------\n",
       "           fc2: Blob(1200 x 100)\n",
       "............................................................\n",
       " *** InnerProductLayer(out)\n",
       "    Inputs ----------------------------\n",
       "           fc2: Blob(1200 x 100)\n",
       "    Outputs ---------------------------\n",
       "           out: Blob(62 x 100)\n",
       "............................................................\n",
       " *** SoftmaxLossLayer(loss)\n",
       "    Inputs ----------------------------\n",
       "           out: Blob(62 x 100)\n",
       "         label: Blob(1 x 100)\n",
       "************************************************************\n",
       ",:valid_net=>************************************************************\n",
       "          NAME: SVHN-validation-prediction\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 5 layers\n",
       "............................................................\n",
       " *** AsyncHDF5DataLayer(validation-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(20 x 20 x 1 x 785)\n",
       "         label: Blob(1 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(fc1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(20 x 20 x 1 x 785)\n",
       "    Outputs ---------------------------\n",
       "           fc1: Blob(2400 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(fc2)\n",
       "    Inputs ----------------------------\n",
       "           fc1: Blob(2400 x 785)\n",
       "    Outputs ---------------------------\n",
       "           fc2: Blob(1200 x 785)\n",
       "............................................................\n",
       " *** InnerProductLayer(out)\n",
       "    Inputs ----------------------------\n",
       "           fc2: Blob(1200 x 785)\n",
       "    Outputs ---------------------------\n",
       "           out: Blob(62 x 785)\n",
       "............................................................\n",
       " *** AccuracyLayer(validation)\n",
       "    Inputs ----------------------------\n",
       "           out: Blob(62 x 785)\n",
       "         label: Blob(1 x 785)\n",
       "************************************************************\n",
       ",:base_dir=>\"snapshot_mlp_dropout_2400_1200_0.1_0.3\",:validLabels=>{0,1,5,7,14,19,21,15,24,24  …  21,46,21,59,14,55,22,21,14,40}},(2400,1200,0.1,0.3),0.6585987261146496)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model, best_cfg, best_score = gridtune(estfun, evalfun, nunits_l1, nunits_l2, base_mom, base_lr; verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-Aug 23:13:46:INFO:root:Constructing net SVHN-Test on CPUBackend...\n",
      "20-Aug 23:13:46:INFO:root:Topological sorting 7 layers...\n",
      "20-Aug 23:13:46:INFO:root:Setup layers...\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(fc1): sharing weights and bias\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(fc2): sharing weights and bias\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(out): sharing weights and bias\n",
      "20-Aug 23:13:46:INFO:root:Network constructed!\n",
      "20-Aug 23:13:46:INFO:root:Constructing net SVHN-Test on CPUBackend...\n",
      "20-Aug 23:13:46:INFO:root:Topological sorting 7 layers...\n",
      "20-Aug 23:13:46:INFO:root:Setup layers...\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(fc1): sharing weights and bias\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(fc2): sharing weights and bias\n",
      "20-Aug 23:13:46:DEBUG:root:InnerProductLayer(out): sharing weights and bias\n",
      "20-Aug 23:13:46:INFO:root:Network constructed!\n",
      "************************************************************\n",
      "          NAME: SVHN-Test\n",
      "       BACKEND: CPUBackend\n",
      "  ARCHITECTURE: 7 layers\n",
      "............................................................\n",
      " *** AsyncHDF5DataLayer(hdf5-data)\n",
      "    Outputs ---------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "         label: Blob(1 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_in)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc1)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           fc1: Blob(2400 x 6220)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_fc1)\n",
      "    Inputs ----------------------------\n",
      "           fc1: Blob(2400 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc2)\n",
      "    Inputs ----------------------------\n",
      "           fc1: Blob(2400 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           fc2: Blob(1200 x 6220)\n",
      "............................................................\n",
      " *** InnerProductLayer(out)\n",
      "    Inputs ----------------------------\n",
      "           fc2: Blob(1200 x 6220)\n",
      "    Outputs ---------------------------\n",
      "           out: Blob(62 x 6220)\n",
      "............................................................\n",
      " *** SoftmaxLayer(prob)\n",
      "    Inputs ----------------------------\n",
      "           out: Blob(62 x 6220)\n",
      "    Outputs ---------------------------\n",
      "          prob: Blob(62 x 6220)\n",
      "************************************************************\n",
      "\n",
      "20-Aug 23:13:46:INFO:root:Loading existing model from snapshot_mlp_dropout_2400_1200_0.1_0.3/snapshot-004000.jld\n",
      "20-Aug 23:13:46:DEBUG:root:Loading parameters for layer fc1\n",
      "20-Aug 23:13:46:DEBUG:root:Loading parameters for layer fc2\n",
      "20-Aug 23:13:46:DEBUG:root:Loading parameters for layer out\n",
      "20-Aug 23:13:46:DEBUG:root:Init network SVHN-Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "785-element Array{Float64,1}:\n",
       " 14.0\n",
       "  9.0\n",
       " 44.0\n",
       " 22.0\n",
       " 14.0\n",
       " 19.0\n",
       " 21.0\n",
       " 28.0\n",
       " 24.0\n",
       " 24.0\n",
       "  9.0\n",
       " 22.0\n",
       " 17.0\n",
       "  ⋮  \n",
       " 20.0\n",
       " 14.0\n",
       " 21.0\n",
       " 46.0\n",
       " 21.0\n",
       " 11.0\n",
       " 14.0\n",
       "  4.0\n",
       " 45.0\n",
       " 13.0\n",
       " 14.0\n",
       " 40.0"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend = best_model[:net].backend\n",
    "common_layers = best_model[:net].layers[2:end-1]\n",
    "test_batch = size(testImages)[4]\n",
    "\n",
    "test_data_layer = AsyncHDF5DataLayer(source=\"data/test.txt\", batch_size=test_batch, shuffle=false)\n",
    "softmax_layer = SoftmaxLayer(name=\"prob\", tops=[:prob], bottoms=[:out])\n",
    "test_net = Net(\"SVHN-Test\", backend, [test_data_layer, common_layers..., softmax_layer])\n",
    "\n",
    "println(test_net)\n",
    "pred = predict(test_net, best_model[:base_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = joinpath(path, \"juliaNN-Dropout-Submission.csv\")\n",
    "dic_rev = Dict(zip(values(dic), keys(dic)))\n",
    "labelsInfoTest[:Class] = map(k -> dic_rev[k], pred)\n",
    "writetable(filename, labelsInfoTest[:,[:ID, :Class]], separator=',', header=true)\n",
    "run(`sed -i '' 's/\"//g' $(filename)`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
